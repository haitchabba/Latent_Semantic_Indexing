{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-2.1.0.tar.gz (116 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.14.40-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.18.0,>=1.17.40\n",
      "  Using cached botocore-1.17.40-py2.py3-none-any.whl (6.5 MB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from botocore<1.18.0,>=1.17.40->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110324 sha256=727049069b9b136fd1dd725191bd560b988d694fe9d8aa5c74667997a8596823\n",
      "  Stored in directory: c:\\users\\ihassan\\appdata\\local\\pip\\cache\\wheels\\56\\b5\\6d\\86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648\n",
      "Successfully built smart-open\n",
      "Installing collected packages: Cython, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.15\n",
      "    Uninstalling Cython-0.29.15:\n",
      "      Successfully uninstalled Cython-0.29.15\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed Cython-0.29.14 boto3-1.14.40 botocore-1.17.40 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-2.3.2-cp37-cp37m-win_amd64.whl (9.3 MB)\n",
      "Collecting thinc==7.4.1\n",
      "  Downloading thinc-7.4.1-cp37-cp37m-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from spacy) (4.42.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from spacy) (45.2.0.post20200210)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.2-cp37-cp37m-win_amd64.whl (20 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.2-cp37-cp37m-win_amd64.whl (105 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.2-cp37-cp37m-win_amd64.whl (179 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.7.1.tar.gz (22 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp37-cp37m-win_amd64.whl (5.0 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.3-cp37-cp37m-win_amd64.whl (32 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ihassan\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\n",
      "Building wheels for collected packages: wasabi\n",
      "  Building wheel for wasabi (setup.py): started\n",
      "  Building wheel for wasabi (setup.py): finished with status 'done'\n",
      "  Created wheel for wasabi: filename=wasabi-0.7.1-py3-none-any.whl size=20841 sha256=1c0deb35002f8be03ceedd7a4e94aecfa2e0f2d1cb570a486419ffa590544c22\n",
      "  Stored in directory: c:\\users\\ihassan\\appdata\\local\\pip\\cache\\wheels\\dc\\5e\\d4\\727b6213e9ebec502ff1bf5998f4a83fef87c3aace8a492243\n",
      "Successfully built wasabi\n",
      "Installing collected packages: cymem, murmurhash, preshed, srsly, blis, plac, wasabi, catalogue, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 wasabi-0.7.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iHASSAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-1252\n"
     ]
    }
   ],
   "source": [
    "import chardet    \n",
    "rawdata = open('Data/RealEstate.csv', 'rb').read()\n",
    "result = chardet.detect(rawdata)\n",
    "charenc = result['encoding']\n",
    "print(charenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_text_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smart Business Real Estate is proud to present...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My Island Real Estate is proud to present 1 Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New on the market, amazing opportunity for inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Bedroom Apartment in Marina Diamond 2\\n\\n- 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No penalty for early contract termination!&lt;br ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>Castles Plaza Real Estate is pleased to bring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>The Noble House Real Estate is proud to presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>Better Homes would like to present this type 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>Morgan‚Äôs International Realty is proud to pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>3 Bedroom in Al Reem 1, Arabian Ranches, direc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>761 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      property_text_en\n",
       "0    Smart Business Real Estate is proud to present...\n",
       "1    My Island Real Estate is proud to present 1 Be...\n",
       "2    New on the market, amazing opportunity for inv...\n",
       "3    1 Bedroom Apartment in Marina Diamond 2\\n\\n- 1...\n",
       "4    No penalty for early contract termination!<br ...\n",
       "..                                                 ...\n",
       "756  Castles Plaza Real Estate is pleased to bring ...\n",
       "757  The Noble House Real Estate is proud to presen...\n",
       "758  Better Homes would like to present this type 2...\n",
       "759  Morgan‚Äôs International Realty is proud to pr...\n",
       "760  3 Bedroom in Al Reem 1, Arabian Ranches, direc...\n",
       "\n",
       "[761 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Data/RealEstate.csv', encoding = 'Windows-1252', error_bad_lines=False)\n",
    "data_text = data[['property_text_en']]\n",
    "documents = data_text\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761\n",
      "                                    property_text_en\n",
      "0  Smart Business Real Estate is proud to present...\n",
      "1  My Island Real Estate is proud to present 1 Be...\n",
      "2  New on the market, amazing opportunity for inv...\n",
      "3  1 Bedroom Apartment in Marina Diamond 2\\n\\n- 1...\n",
      "4  No penalty for early contract termination!<br ...\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [smart, busi, real, estat, proud, present, bed...\n",
       "1    [island, real, estat, proud, present, bedroom,...\n",
       "2    [market, amaz, opportun, investor, apart, offe...\n",
       "3    [bedroom, apart, marina, diamond, bedroom, apa...\n",
       "4    [penalti, earli, contract, termin, price, vari...\n",
       "5    [properti, detail, ndubai, marina, nelit, resi...\n",
       "6    [smart, busi, real, estat, pleas, offer, bedro...\n",
       "7    [real, estat, proud, present, spacious, bedroo...\n",
       "8    [elit, resid, tower, dubai, marina, offer, spe...\n",
       "9    [forest, proud, offer, apart, heart, marina, r...\n",
       "Name: property_text_en, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['property_text_en'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 appoint\n",
      "1 area\n",
      "2 avail\n",
      "3 bedroom\n",
      "4 build\n",
      "5 busi\n",
      "6 cafe\n",
      "7 close\n",
      "8 court\n",
      "9 current\n",
      "10 desir\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (4, 1),\n",
       " (8, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (36, 1),\n",
       " (40, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (73, 1),\n",
       " (75, 1),\n",
       " (93, 2),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (103, 1),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (121, 1),\n",
       " (125, 1),\n",
       " (131, 1),\n",
       " (133, 1),\n",
       " (148, 1),\n",
       " (157, 1),\n",
       " (158, 1),\n",
       " (176, 1),\n",
       " (236, 1),\n",
       " (240, 3),\n",
       " (245, 3),\n",
       " (256, 1),\n",
       " (265, 1),\n",
       " (270, 1),\n",
       " (271, 1),\n",
       " (272, 1),\n",
       " (273, 1),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (277, 1),\n",
       " (281, 1),\n",
       " (282, 2),\n",
       " (284, 1),\n",
       " (286, 1),\n",
       " (287, 1),\n",
       " (288, 2),\n",
       " (289, 1),\n",
       " (290, 1),\n",
       " (291, 1),\n",
       " (292, 1),\n",
       " (293, 1),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 1),\n",
       " (299, 1),\n",
       " (300, 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.16560989594171266),\n",
      " (1, 0.0883939922999352),\n",
      " (2, 0.290144256381834),\n",
      " (3, 0.20226482670613383),\n",
      " (4, 0.08773817517147281),\n",
      " (5, 0.26266018616446263),\n",
      " (6, 0.17454218619136122),\n",
      " (7, 0.2504641846696627),\n",
      " (8, 0.09177313551730533),\n",
      " (9, 0.12938093112657487),\n",
      " (10, 0.14280435809185013),\n",
      " (11, 0.16355149318508566),\n",
      " (12, 0.10389985392843741),\n",
      " (13, 0.05138473652330455),\n",
      " (14, 0.1862620802577546),\n",
      " (15, 0.25834302304833406),\n",
      " (16, 0.10065593052880868),\n",
      " (17, 0.204097621116352),\n",
      " (18, 0.21410021918044642),\n",
      " (19, 0.2542876716609471),\n",
      " (20, 0.10730914262234359),\n",
      " (21, 0.20226482670613383),\n",
      " (22, 0.06808766810207303),\n",
      " (23, 0.21410021918044642),\n",
      " (24, 0.12190873722998359),\n",
      " (25, 0.24341632357714293),\n",
      " (26, 0.1757474946798856),\n",
      " (27, 0.10515806302177604),\n",
      " (28, 0.09352895315113317),\n",
      " (29, 0.08180905908473983),\n",
      " (30, 0.0883939922999352),\n",
      " (31, 0.2162936226111519),\n",
      " (32, 0.17697492015036095)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.015*\"live\" + 0.013*\"walk\" + 0.013*\"larg\" + 0.012*\"fulli\" + 0.012*\"bathroom\" + 0.012*\"balconi\" + 0.010*\"suit\" + 0.009*\"equip\" + 0.009*\"spacious\" + 0.009*\"swim\"\n",
      "Topic: 1 \n",
      "Words: 0.018*\"luxuri\" + 0.014*\"residenti\" + 0.014*\"jumeirah\" + 0.013*\"palm\" + 0.011*\"world\" + 0.011*\"elit\" + 0.010*\"equip\" + 0.009*\"develop\" + 0.009*\"unit\" + 0.009*\"diamond\"\n",
      "Topic: 2 \n",
      "Words: 0.021*\"princess\" + 0.017*\"residenti\" + 0.016*\"world\" + 0.015*\"tallest\" + 0.011*\"high\" + 0.011*\"palm\" + 0.011*\"balconi\" + 0.010*\"rent\" + 0.010*\"live\" + 0.010*\"jumeirah\"\n",
      "Topic: 3 \n",
      "Words: 0.017*\"live\" + 0.017*\"luxuri\" + 0.016*\"elit\" + 0.014*\"bathroom\" + 0.011*\"residenti\" + 0.010*\"rent\" + 0.009*\"space\" + 0.009*\"high\" + 0.009*\"intern\" + 0.008*\"fulli\"\n",
      "Topic: 4 \n",
      "Words: 0.024*\"palm\" + 0.021*\"jumeirah\" + 0.018*\"penthous\" + 0.015*\"bathroom\" + 0.012*\"arab\" + 0.011*\"walk\" + 0.010*\"equip\" + 0.010*\"world\" + 0.009*\"high\" + 0.009*\"grind\"\n",
      "Topic: 5 \n",
      "Words: 0.016*\"elit\" + 0.013*\"walk\" + 0.011*\"high\" + 0.011*\"world\" + 0.010*\"noffic\" + 0.010*\"live\" + 0.010*\"broker\" + 0.010*\"provid\" + 0.009*\"servic\" + 0.009*\"swim\"\n",
      "Topic: 6 \n",
      "Words: 0.017*\"mall\" + 0.016*\"princess\" + 0.014*\"beach\" + 0.014*\"live\" + 0.014*\"world\" + 0.013*\"walk\" + 0.011*\"communiti\" + 0.010*\"jumeirah\" + 0.010*\"residenti\" + 0.009*\"restaur\"\n",
      "Topic: 7 \n",
      "Words: 0.019*\"walk\" + 0.015*\"beach\" + 0.014*\"swim\" + 0.013*\"elit\" + 0.012*\"shop\" + 0.011*\"station\" + 0.010*\"metro\" + 0.010*\"facil\" + 0.010*\"rent\" + 0.010*\"access\"\n",
      "Topic: 8 \n",
      "Words: 0.024*\"residenti\" + 0.019*\"walk\" + 0.014*\"elit\" + 0.014*\"tallest\" + 0.013*\"station\" + 0.013*\"exclus\" + 0.012*\"shop\" + 0.011*\"unit\" + 0.011*\"swim\" + 0.011*\"servic\"\n",
      "Topic: 9 \n",
      "Words: 0.023*\"ranch\" + 0.021*\"villa\" + 0.018*\"studi\" + 0.017*\"reem\" + 0.017*\"arabian\" + 0.016*\"bathroom\" + 0.016*\"communiti\" + 0.015*\"live\" + 0.013*\"garden\" + 0.011*\"type\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.011*\"drive\" + 0.007*\"nrera\" + 0.007*\"noffic\" + 0.006*\"list\" + 0.006*\"ncall\" + 0.006*\"sell\" + 0.006*\"sale\" + 0.005*\"jumeirah\" + 0.005*\"landlord\" + 0.005*\"road\"\n",
      "Topic: 1 Word: 0.008*\"villa\" + 0.007*\"exclus\" + 0.006*\"princess\" + 0.006*\"servic\" + 0.006*\"residenti\" + 0.005*\"agent\" + 0.005*\"manag\" + 0.005*\"communiti\" + 0.005*\"unit\" + 0.005*\"ranch\"\n",
      "Topic: 2 Word: 0.007*\"princess\" + 0.007*\"facil\" + 0.007*\"jumeirah\" + 0.007*\"recreat\" + 0.007*\"billiard\" + 0.006*\"podium\" + 0.006*\"tabl\" + 0.006*\"children\" + 0.006*\"indoor\" + 0.006*\"high\"\n",
      "Topic: 3 Word: 0.008*\"studi\" + 0.007*\"walk\" + 0.007*\"shop\" + 0.006*\"station\" + 0.006*\"garden\" + 0.006*\"avail\" + 0.005*\"arrang\" + 0.005*\"beach\" + 0.005*\"short\" + 0.005*\"master\"\n",
      "Topic: 4 Word: 0.010*\"arrang\" + 0.009*\"head\" + 0.008*\"altern\" + 0.008*\"select\" + 0.007*\"garden\" + 0.007*\"eleg\" + 0.007*\"inform\" + 0.007*\"avail\" + 0.007*\"dacha\" + 0.007*\"elit\"\n",
      "Topic: 5 Word: 0.008*\"tallest\" + 0.008*\"elit\" + 0.008*\"tameer\" + 0.007*\"world\" + 0.007*\"residenti\" + 0.007*\"nabout\" + 0.007*\"princess\" + 0.006*\"storey\" + 0.006*\"district\" + 0.006*\"exclus\"\n",
      "Topic: 6 Word: 0.009*\"elit\" + 0.006*\"equip\" + 0.006*\"fulli\" + 0.005*\"modern\" + 0.005*\"free\" + 0.005*\"minut\" + 0.005*\"servic\" + 0.005*\"provid\" + 0.005*\"hour\" + 0.005*\"develop\"\n",
      "Topic: 7 Word: 0.009*\"diamond\" + 0.008*\"complet\" + 0.006*\"hand\" + 0.006*\"ranch\" + 0.006*\"citi\" + 0.006*\"communiti\" + 0.006*\"mall\" + 0.005*\"repres\" + 0.005*\"residenti\" + 0.005*\"preced\"\n",
      "Topic: 8 Word: 0.011*\"sothebi\" + 0.011*\"realti\" + 0.009*\"intern\" + 0.009*\"gulf\" + 0.007*\"applianc\" + 0.006*\"luxuri\" + 0.006*\"extraordinari\" + 0.006*\"search\" + 0.006*\"wide\" + 0.006*\"reve\"\n",
      "Topic: 9 Word: 0.007*\"luxuri\" + 0.006*\"intern\" + 0.006*\"live\" + 0.006*\"jumeirah\" + 0.005*\"world\" + 0.005*\"palm\" + 0.005*\"featur\" + 0.005*\"nthe\" + 0.005*\"station\" + 0.005*\"elit\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
